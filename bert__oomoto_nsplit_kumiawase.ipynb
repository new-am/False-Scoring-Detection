{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05a3c0d",
   "metadata": {},
   "source": [
    "# n分割/大元からランダムにn件選びそこから更にわざと誤採点を作る（位置指定なし）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55a88a",
   "metadata": {},
   "source": [
    "# クラスタリングを行い,学習データを選ぶ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85c2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import preprocessing\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "# 引数は(解答の文章ベクトル、解答の文章、クラスタ数)\n",
    "# 学習データになるインデックスが先に格納され、そのあとに開発orテストデータとなるインデックスが格納されたリストを返す\n",
    "def clustering(cls_token_vec, answer_text, cluster_num):\n",
    "    # データ数を確認する\n",
    "    student_num = len(answer_text)\n",
    "    print(\"問題数　\" + str(student_num))\n",
    "    \n",
    "    if cluster_num < 1:\n",
    "        \n",
    "        clustering_method = 1\n",
    "        \n",
    "        # 解答文ベクトルをpandasのデータフレームに格納\n",
    "        df = pd.DataFrame(cls_token_vec)\n",
    "\n",
    "        # 階層型クラスタリングの実施\n",
    "        if clustering_method == 0:\n",
    "            # ウォード法 x ユークリッド距離\n",
    "            print(\"---Ward法でクラスタリング---\\n\")\n",
    "            linkage_result = linkage(df, method='ward', metric='euclidean')\n",
    "        elif clustering_method == 1:\n",
    "            print(\"---最短距離法でクラスタリング---\\n\")\n",
    "            linkage_result = linkage(df, method='single', metric='euclidean')\n",
    "        elif clustering_method == 2: \n",
    "            print(\"---最長距離法でクラスタリング---\\n\")\n",
    "            linkage_result = linkage(df, method='complete', metric='euclidean')\n",
    "        elif clustering_method == 3: \n",
    "            print(\"---郡平均法でクラスタリング---\\n\")\n",
    "            linkage_result = linkage(df, method='average', metric='euclidean')\n",
    "\n",
    "        # クラスタ分けする閾値を決める\n",
    "        print(\"クラスタリングの閾値d(ユークリッド距離)の倍率を入力してください(0<d≦1.0)\")\n",
    "        #thresh = float(input('>>>  '))\n",
    "        #thresh = 0.05\n",
    "        thresh = cluster_num\n",
    "        print(\"thresh = \" + str(thresh))\n",
    "        threshold = thresh * np.max(linkage_result[:, 2])\n",
    "\n",
    "        # 階層型クラスタリングの可視化\n",
    "        plt.figure(num=None, figsize=(6, 3), dpi=300, facecolor='w', edgecolor='k')\n",
    "        dendrogram(linkage_result, labels=answer_text, color_threshold=threshold)\n",
    "        plt.show()\n",
    "        #plt.savefig('figure_dendrogram.png')\n",
    "\n",
    "        # クラスタリング結果のクラスタ番号を取得\n",
    "        clustered = fcluster(linkage_result, threshold, criterion='distance')\n",
    "\n",
    "        # クラスタリング結果を表示\n",
    "        cluster_num = np.max(clustered)\n",
    "        \n",
    "    else:\n",
    "        print(\"---k-means法でクラスタリング---\\n\")\n",
    "        # クラスタ番号は0から\n",
    "        \n",
    "        \n",
    "        clustered = KMeans(n_clusters=cluster_num, init='k-means++', n_init=10).fit_predict(cls_token_vec)   # k-means++で初期位置を決める\n",
    "\n",
    "    # クラスタ番号が同じインデックスを集める\n",
    "    cluster_index = [[] for _ in range(cluster_num)]\n",
    "    for index, num in enumerate(clustered):\n",
    "        cluster_index[num].append(index)\n",
    "\n",
    "    return cal_center(cls_token_vec ,cluster_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c855b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy import stats\n",
    "def simple(test_cls,cluster_num):\n",
    "    cls_sub = np.array(test_cls)\n",
    "    print(\"クラスタリング開始\")\n",
    "    clustered = KMeans(n_clusters=cluster_num, init='k-means++', n_init=10).fit_predict(cls_sub)   # k-means++で初期位置を決める\n",
    "    print(\"クラスタリング終了\")\n",
    "    # クラスタ番号が同じインデックスを集める\n",
    "    cluster_index = [[] for _ in range(cluster_num)]\n",
    "    for index, num in enumerate(clustered):\n",
    "        cluster_index[num].append(index)\n",
    "\n",
    "    return cal_center(cls_sub.tolist() ,cluster_index)\n",
    "def uncertainty(test_un,cluster_num):\n",
    "    test_un = np.max(test_un,axis = 1)\n",
    "    un_dict = {}\n",
    "    for index,i in enumerate(test_un):\n",
    "        un_dict[index] = i\n",
    "    \n",
    "    result = sorted(un_dict.items(), key=lambda x:x[1],reverse= False)\n",
    "    print(result[:10])\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for i in range(cluster_num):\n",
    "        train_list.append(int(result[i][0]))\n",
    "    \n",
    "    for i in range(len(test_un)):\n",
    "        if(i not in train_list):\n",
    "            test_list.append(i)\n",
    "    \n",
    "    random.shuffle(train_list)\n",
    "    random.shuffle(test_list)\n",
    "    train_list.extend(test_list)\n",
    "    \n",
    "    return train_list\n",
    "\n",
    "def mergin(test_un,cluster_num):\n",
    "    un_dict = {}\n",
    "    test_un = np.array(test_un)\n",
    "    a_argsorted = np.argsort(test_un, axis=1)\n",
    "    for index,i in enumerate(test_un):\n",
    "        un_dict[index] = (test_un[index][a_argsorted[index][-1]] -  test_un[index][a_argsorted[index][-2]])\n",
    "    \n",
    "    result = sorted(un_dict.items(), key=lambda x:x[1],reverse= False)\n",
    "    print(result[:10])\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for i in range(cluster_num):\n",
    "        train_list.append(int(result[i][0]))\n",
    "    \n",
    "    for i in range(len(test_un)):\n",
    "        if(i not in train_list):\n",
    "            test_list.append(i)\n",
    "    \n",
    "    random.shuffle(train_list)\n",
    "    random.shuffle(test_list)\n",
    "    train_list.extend(test_list)\n",
    "    \n",
    "    return train_list\n",
    "\n",
    "def badge(test_un,test_cls,predict_list,cluster_num,label_num = 4):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    test_un = np.array(test_un,dtype = np.float32)\n",
    "    test_cls = np.array(test_cls,dtype = np.float32)\n",
    "    grad_list = []\n",
    "    for index,i in enumerate(test_cls):\n",
    "        #if(index % 1000 == 0):\n",
    "            #print(index)\n",
    "            #print(\"{}:{}\".format(len(test_un),grad_list.shape))\n",
    "        #print(grad_list.shape)\n",
    "        grad_sub = []\n",
    "        for j in range(label_num):\n",
    "            if(j == predict_list[index]):\n",
    "                grad_sub.extend(((test_un[index][j] - 1) * test_cls[index]))\n",
    "            else:\n",
    "                grad_sub.extend(((test_un[index][j]) * test_cls[index]))\n",
    "        grad_list.append(np.array(grad_sub,dtype = np.float32))\n",
    "    \n",
    "    #grad_list = np.array(grad_list)\n",
    "    ind = np.argmax([np.linalg.norm(s, 2) for s in grad_list])\n",
    "    mu = [grad_list[ind]]\n",
    "    indsAll = [ind]\n",
    "    centInds = [0.] * len(grad_list)\n",
    "    cent = 0\n",
    "    print('#Samps\\tTotal Distance')\n",
    "    while len(mu) < cluster_num:\n",
    "        if len(mu) == 1:\n",
    "            D2 = pairwise_distances(grad_list, mu).ravel().astype(float)\n",
    "        else:\n",
    "            newD = pairwise_distances(grad_list, [mu[-1]]).ravel().astype(float)\n",
    "            for i in range(len(grad_list)):\n",
    "                if D2[i] >  newD[i]:\n",
    "                    centInds[i] = cent\n",
    "                    D2[i] = newD[i]\n",
    "        if sum(D2) == 0.0: pdb.set_trace()\n",
    "        D2 = D2.ravel().astype(float)\n",
    "        Ddist = (D2 ** 2)/ sum(D2 ** 2)\n",
    "        customDist = stats.rv_discrete(name='custm', values=(np.arange(len(D2)), Ddist))\n",
    "        ind = customDist.rvs(size=1)[0]\n",
    "        while ind in indsAll: ind = customDist.rvs(size=1)[0]\n",
    "        mu.append(grad_list[ind])\n",
    "        indsAll.append(ind)\n",
    "        cent += 1\n",
    "        \n",
    "    train_list.extend(indsAll)\n",
    "    for i in range(len(test_cls)):\n",
    "        if(i not in train_list):\n",
    "            test_list.append(i)\n",
    "            \n",
    "    random.shuffle(train_list)\n",
    "    random.shuffle(test_list)\n",
    "    train_list.extend(test_list)\n",
    "    \n",
    "    return train_list\n",
    "\n",
    "def entropy(test_un,cluster_num):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    prob_log = np.log(test_un)\n",
    "    entropy_sum = (np.array(test_un) * np.array(prob_log)).sum(1)\n",
    "    prob_dict = {}\n",
    "    for i in range(len(test_un)):\n",
    "        prob_dict[i] = entropy_sum[i]\n",
    "        \n",
    "    result = sorted(prob_dict.items(), key=lambda x:x[1],reverse= False)\n",
    "    \n",
    "    print(result[:10])\n",
    "\n",
    "    for i in range(cluster_num):\n",
    "        train_list.append(int(result[i][0]))\n",
    "    \n",
    "    for i in range(len(test_un)):\n",
    "        if(i not in train_list):\n",
    "            test_list.append(i)\n",
    "    \n",
    "    random.shuffle(train_list)\n",
    "    random.shuffle(test_list)\n",
    "    train_list.extend(test_list)\n",
    "    \n",
    "    return train_list\n",
    "\n",
    "def core_set(train_cls,test_cls,cluster_num):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    dis_list = []\n",
    "    \n",
    "    for i in range(len(test_cls)):\n",
    "        min_dis = 1000000\n",
    "        for j in range(len(train_cls)):\n",
    "            dis = np.linalg.norm(test_cls[i] - np.array(train_cls[j]), ord=2)\n",
    "            if(min_dis > dis):\n",
    "                min_dis = dis\n",
    "        dis_list.append(min_dis)\n",
    "        \n",
    "    train_list.append(np.argmax(dis_list))\n",
    "    \n",
    "    while(len(train_list) != cluster_num):\n",
    "        if(len(train_list) % 100 == 0):\n",
    "            print(len(train_list))\n",
    "        for i in range(len(test_cls)):\n",
    "            dis = np.linalg.norm(test_cls[i] - np.array(test_cls[train_list[-1]]), ord=2)\n",
    "            if(dis_list[i] > dis):\n",
    "                dis_list[i] = dis\n",
    "                \n",
    "        train_list.append(np.argmax(dis_list))\n",
    "        \n",
    "    for i in range(len(test_cls)):\n",
    "        if(i not in train_list):\n",
    "            test_list.append(i)\n",
    "    \n",
    "    random.shuffle(train_list)\n",
    "    random.shuffle(test_list)\n",
    "    train_list.extend(test_list)\n",
    "    \n",
    "    return train_list\n",
    "def core_set_new(train_cls,test_cls,cluster_num):\n",
    "    \n",
    "    lb_flag = []\n",
    "    for i in range(len(train_cls)):\n",
    "        lb_flag.append(True)\n",
    "    for i in range(len(test_cls)):\n",
    "        lb_flag.append(False)\n",
    "    embedding  = train_cls\n",
    "    embedding.extend(test_cls)\n",
    "    embedding = np.array(embedding)\n",
    "\n",
    "    print('calculate distance matrix')\n",
    "    dist_mat = np.matmul(embedding, embedding.transpose())\n",
    "    sq = np.array(dist_mat.diagonal()).reshape(len(embedding), 1)\n",
    "    dist_mat *= -2\n",
    "    dist_mat += sq\n",
    "    dist_mat += sq.transpose()\n",
    "    dist_mat = np.sqrt(dist_mat)\n",
    "\n",
    "    print('calculate greedy solution')\n",
    "    mat = dist_mat[~lb_flag, :][:, lb_flag]\n",
    "\n",
    "    for i in range(cluster_num):\n",
    "        if i%100 == 0:\n",
    "            print('greedy solution {}/{}'.format(i, cluster_num))\n",
    "        mat_min = mat.min(axis=1)\n",
    "        q_idx_ = mat_min.argmax()\n",
    "        q_idx = np.arange(len(embedding))[~lb_flag][q_idx_]\n",
    "        lb_flag[q_idx] = True\n",
    "        mat = np.delete(mat, q_idx_, 0)\n",
    "        mat = np.append(mat, dist_mat[~lb_flag, q_idx][:, None], axis=1)\n",
    "        \n",
    "    print(len(mat))\n",
    "    print(mat)\n",
    "    \n",
    "def core_set_pair(train_cls,test_cls,cluster_num,device):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    dis_list = []\n",
    "    train_cls = torch.tensor(train_cls ,dtype = torch.float32,device = device)\n",
    "    test_cls = torch.tensor(test_cls ,dtype = torch.float32,device = device)\n",
    "    print(\"距離計算\")\n",
    "    dist_ctr = torch.cdist(test_cls,train_cls).cpu()\n",
    "    #dist_ctr = pairwise_distances(np.array(test_cls), np.array(train_cls))\n",
    "    \n",
    "    \n",
    "    \n",
    "    min_dist = torch.amin(dist_ctr,1)\n",
    "    \n",
    "    del train_cls\n",
    "    del dist_ctr\n",
    "    torch.cuda.empty_cache()\n",
    "    #np.amin(dist_ctr, axis=1)\n",
    "    print(\"サンプリング開始\")\n",
    "    \n",
    "    torch_batch = len(test_cls)\n",
    "    while(len(train_list) != cluster_num):\n",
    "        train_list.append(torch.argmax(min_dist).tolist())\n",
    "        \n",
    "        for i in range(0,len(test_cls),torch_batch):\n",
    "            dist = torch.cdist(test_cls[i:i+torch_batch],test_cls[train_list[-1]:train_list[-1] + 1])\n",
    "            for index,j in enumerate(torch.flatten(dist).tolist()):\n",
    "                if(min_dist[i + index] > j):\n",
    "                    min_dist[i + index] = j\n",
    "     \n",
    "    #while(len(train_list) != cluster_num):\n",
    "       # print(len(train_list))\n",
    "       # train_list.append(np.argmax(min_dist))\n",
    "        #dist_ctr = pairwise_distances(np.array(test_cls), np.array(test_cls)[[train_list[-1]],:])\n",
    "       # for i in range(len(test_cls)):\n",
    "           # if(min_dist[i] > dist_ctr[i][0]):\n",
    "               # min_dist[i] = dist_ctr[i][0]\n",
    "    \n",
    "        \n",
    "    for i in range(len(test_cls)):\n",
    "        if(i not in train_list):\n",
    "            test_list.append(i)\n",
    "    \n",
    "    random.shuffle(train_list)\n",
    "    random.shuffle(test_list)\n",
    "    train_list.extend(test_list)\n",
    "    \n",
    "    del test_cls\n",
    "    del dist \n",
    "    del min_dist\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return train_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de8fc8",
   "metadata": {},
   "source": [
    "# クラスタリングの重心を決める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb8f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 重心を求める関数 (文章ベクトル、クラスタの番号とインデックスを結びつけたリスト)\n",
    "# 学習データになるインデックスが先に格納され、そのあとに開発orテストデータとなるインデックスが格納されたリストを返す\n",
    "    \n",
    "    \n",
    "def cal_center(cls_token_vec ,cluster_index):\n",
    "    \n",
    "    # クラスターの中心座標（重心）を求める\n",
    "    center_vec = []\n",
    "    for i in range(len(cluster_index)):\n",
    "        total = np.zeros_like(cls_token_vec[0])   # 解答ベクトルと同じ形の配列を用意する\n",
    "        for j in cluster_index[i]:\n",
    "            total += cls_token_vec[j]\n",
    "        center_vec.append(total / len(cluster_index[i]))\n",
    "\n",
    "    # 重心に一番近い解答を求める\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for i, vec in enumerate(center_vec):\n",
    "        min_dis = 10000\n",
    "        index = 0\n",
    "        for j in cluster_index[i]:\n",
    "            test_list.append(j)\n",
    "            # 重心とのユークリッド距離を求める\n",
    "            dis = np.linalg.norm(vec - cls_token_vec[j], ord=2)\n",
    "            if (min_dis > dis):\n",
    "                index = j\n",
    "                min_dis = dis\n",
    "        train_list.append(index)\n",
    "        test_list.remove(index)\n",
    "        \n",
    "    random.shuffle(test_list)\n",
    "    train_list.extend(test_list)\n",
    "    \n",
    "    return train_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26e19c",
   "metadata": {},
   "source": [
    "# 文章ベクトルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6f86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cls_token_vec(model_name,answer_text,hidden_num = 4):\n",
    "    print(\"文章ベクトルの作成を行います\")\n",
    "    cls_token_vec = []\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.update({\"output_hidden_states\": True})\n",
    "    bert = AutoModel.from_pretrained(model_name,config=config).to(device)\n",
    "    tokenizer_clustering = AutoTokenizer.from_pretrained(model_name)\n",
    "    batch_size = 4\n",
    "    for i in range(len(answer_text)//batch_size): \n",
    "        if(i<len(answer_text)//batch_size-1):\n",
    "            encoded_data_clustering = tokenizer_clustering.batch_encode_plus(answer_text[batch_size*i:batch_size*(i+1)], pad_to_max_length=True, add_special_tokens=True)\n",
    "            outputs = bert(input_ids = torch.tensor(encoded_data_clustering[\"input_ids\"],device = device),attention_mask=torch.tensor(encoded_data_clustering[\"attention_mask\"],device = device), token_type_ids=torch.tensor(encoded_data_clustering[\"token_type_ids\"],device = device))\n",
    "                           #,torch.tensor(encoded_data_clustering[\"token_type_ids\"],device = device))\n",
    "            if(hidden_num > 1):\n",
    "                \n",
    "                sequence_output = torch.cat([outputs[\"hidden_states\"][-1*i][:,0] for i in range(1, hidden_num+1)], dim=1)\n",
    "                cls_token_vec.extend(sequence_output.tolist())\n",
    "            else:\n",
    "                                                           \n",
    "                outputs = outputs[\"hidden_states\"][-2][:,0]\n",
    "                cls_token_vec.extend(outputs.tolist())\n",
    "        else:\n",
    "            encoded_data_clustering = tokenizer_clustering.batch_encode_plus(answer_text[batch_size*i:], pad_to_max_length=True, add_special_tokens=True)\n",
    "            outputs = bert(input_ids =torch.tensor(encoded_data_clustering[\"input_ids\"],device = device),attention_mask=torch.tensor(encoded_data_clustering[\"attention_mask\"],device = device), token_type_ids=torch.tensor(encoded_data_clustering[\"token_type_ids\"],device = device))\n",
    "            if(hidden_num > 1):\n",
    "                sequence_output = torch.cat([outputs[\"hidden_states\"][-1*i][:,0] for i in range(1, hidden_num+1)], dim=1)\n",
    "                cls_token_vec.extend(sequence_output.tolist())\n",
    "            else:\n",
    "                outputs = outputs[\"hidden_states\"][-2][:,0]\n",
    "                cls_token_vec.extend(outputs.tolist())\n",
    "    print(len(cls_token_vec))\n",
    "    cls_token_vec = np.array(cls_token_vec).reshape(len(answer_text),-1).tolist()\n",
    "    print(\"文章ベクトルの作成を終了しました\")\n",
    "    return cls_token_vec\n",
    "\n",
    "def create_words_vec(model_name,answer_text,hidden_num = 1):\n",
    "    print(\"文章ベクトルの作成を行います\")\n",
    "    words_vec = []\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.update({\"output_hidden_states\": True})\n",
    "    bert = AutoModel.from_pretrained(model_name,config=config).to(device)\n",
    "    tokenizer_clustering = AutoTokenizer.from_pretrained(model_name)\n",
    "    batch_size = 4\n",
    "    for i in range(len(answer_text)//batch_size): \n",
    "        if(i<len(answer_text)//batch_size-1):\n",
    "            encoded_data_clustering = tokenizer_clustering.batch_encode_plus(answer_text[batch_size*i:batch_size*(i+1)], pad_to_max_length=True, add_special_tokens=True)\n",
    "            outputs = bert(torch.tensor(encoded_data_clustering[\"input_ids\"],device = device),torch.tensor(encoded_data_clustering[\"attention_mask\"],device = device),torch.tensor(encoded_data_clustering[\"token_type_ids\"],device = device))\n",
    "            if(hidden_num > 1):\n",
    "                sequence_output = torch.cat([outputs[\"hidden_states\"][-1*i][:,1:-1] for i in range(1, hidden_num+1)], dim=1)\n",
    "                words_vec.extend(sequence_output.tolist())\n",
    "            else:\n",
    "                outputs = outputs[\"hidden_states\"][-2][:,1:-1]\n",
    "                words_vec.extend(outputs.tolist())\n",
    "        else:\n",
    "            encoded_data_clustering = tokenizer_clustering.batch_encode_plus(answer_text[batch_size*i:], pad_to_max_length=True, add_special_tokens=True)\n",
    "            outputs = bert(torch.tensor(encoded_data_clustering[\"input_ids\"],device = device),torch.tensor(encoded_data_clustering[\"attention_mask\"],device = device),torch.tensor(encoded_data_clustering[\"token_type_ids\"],device = device))\n",
    "            if(hidden_num > 1):\n",
    "                sequence_output = torch.cat([outputs[\"hidden_states\"][-1*i][:,1:-1] for i in range(1, hidden_num+1)], dim=1)\n",
    "                words_vec.extend(sequence_output.tolist())\n",
    "            else:\n",
    "                outputs = outputs[\"hidden_states\"][-2][:,1:-1]\n",
    "                words_vec.extend(outputs.tolist())\n",
    "    print(len(words_vec))\n",
    "    words_vec = np.array(words_vec).reshape(len(answer_text),-1,768).tolist()\n",
    "    print(\"文章ベクトルの作成を終了しました\")\n",
    "    return words_vec\n",
    "\n",
    "def create_cls_token_and_words_vec(model_name,answer_text,hidden_num = 1):\n",
    "    print(\"ベクトルの作成を行います\")\n",
    "    cls_token_vec = []\n",
    "    words_vec = []\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.update({\"output_hidden_states\": True})\n",
    "    bert = AutoModel.from_pretrained(model_name,config=config).to(device)\n",
    "    tokenizer_clustering = AutoTokenizer.from_pretrained(model_name)\n",
    "    for i in range(len(answer_text)): \n",
    "        encoded_data_clustering = tokenizer_clustering(answer_text[i], return_tensors=\"pt\",add_special_tokens=True)\n",
    "        outputs = bert(encoded_data_clustering[\"input_ids\"].to(device),encoded_data_clustering[\"attention_mask\"].to(device),encoded_data_clustering[\"token_type_ids\"].to(device))\n",
    "        if(hidden_num > 1):\n",
    "            sequence_output = torch.cat([outputs[\"hidden_states\"][-1*j][:,0] for j in range(1, hidden_num+1)], dim=1)\n",
    "            word_output =  torch.cat([outputs[\"hidden_states\"][-1*j][:,1:-1] for j in range(1, hidden_num+1)], dim=1)\n",
    "        else:\n",
    "            sequence_output = outputs[\"hidden_states\"][-2][:,0]\n",
    "            word_output = outputs[\"hidden_states\"][-2][:,1:-1].view(-1,768)\n",
    "        cls_token_vec.extend(sequence_output.tolist())\n",
    "        words_vec.append(word_output.tolist())\n",
    "\n",
    "    print(len(cls_token_vec))\n",
    "    print(len(words_vec))\n",
    "    cls_token_vec = np.array(cls_token_vec).reshape(len(answer_text),-1).tolist()\n",
    "    print(\"ベクトルの作成を終了しました\")\n",
    "    return cls_token_vec,words_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033bd9f3",
   "metadata": {},
   "source": [
    "# T-SNEを用いて次元圧縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551aa701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "def change_dimension(cls_token_vec):\n",
    "    print(\"次元圧縮開始\")\n",
    "    cls_token_vec = TSNE(n_components=2, random_state=0).fit_transform(cls_token_vec)\n",
    "    print(\"次元圧縮終了\")\n",
    "    return cls_token_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b210ca",
   "metadata": {},
   "source": [
    "# 採点結果のアルファベットを変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e29be312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_num(graded_score):\n",
    "    nums = []\n",
    "    for score in graded_score:\n",
    "        if score == \"a\":\n",
    "            nums.append(3)\n",
    "        elif score == \"b\":\n",
    "            nums.append(2)\n",
    "        elif score == \"c\":\n",
    "            nums.append(1)\n",
    "        elif score == \"d\":\n",
    "            nums.append(0)\n",
    "            \n",
    "    return nums\n",
    "        \n",
    "def num_to_str(nums):\n",
    "    graded_score = []\n",
    "    for num in nums:\n",
    "        #m = max(num)\n",
    "        if num == 0:\n",
    "            graded_score.append(\"d\")\n",
    "        elif num == 1:\n",
    "            graded_score.append(\"c\")\n",
    "        elif num == 2:\n",
    "            graded_score.append(\"b\")\n",
    "        elif num == 3:\n",
    "            graded_score.append(\"a\")\n",
    "    \n",
    "    return graded_score\n",
    "\n",
    "def num_to_num(nums):\n",
    "    result = []\n",
    "    #kokokaeru\n",
    "    for num in nums:\n",
    "        if num == \"0\":\n",
    "            result.append(0)\n",
    "        elif num == \"1\":\n",
    "            result.append(0)\n",
    "        elif num == \"2\":\n",
    "            result.append(0)\n",
    "        elif num == \"3\":\n",
    "            result.append(1)\n",
    "        elif num == \"4\":\n",
    "            result.append(1)\n",
    "        else :\n",
    "            result.append(1)\n",
    "    return result\n",
    "    \n",
    "def num_to_num_after(nums):\n",
    "    result = []\n",
    "    for num in nums:\n",
    "        if num == 0:\n",
    "            result.append(0)\n",
    "        elif num == 1:\n",
    "            result.append(1)\n",
    "        elif num == 2:\n",
    "            result.append(2)\n",
    "        elif num == 3:\n",
    "            result.append(3)\n",
    "        elif num == 4:\n",
    "            result.append(4)\n",
    "        elif num == 5:\n",
    "            result.append(5)\n",
    "        elif num == 6:\n",
    "            result.append(6)\n",
    "\n",
    "    return result\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f946408-7b1d-4599-9343-69c105aa7e0d",
   "metadata": {},
   "source": [
    "# 学習モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d0a9d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xolovestephi/opt/anaconda3/envs/python38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from transformers import AutoTokenizer,T5ForConditionalGeneration,T5EncoderModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self,MODEL_NAME,r = 4):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        #self.config = AutoConfig.from_pretrained(MODEL_NAME)  config=self.config self.config.hidden_size\n",
    "        #self.config.update({\"output_hidden_states\": True})\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "        self.regressor = nn.Linear(768*4, r)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        nn.init.normal_(self.regressor.weight,mean= 0.0,std = 0.02)\n",
    "        nn.init.zeros_(self.regressor.bias)\n",
    "        \n",
    "    def _get_cls_vec(self, vec):\n",
    "        return vec[:,0,:].view(-1, 768)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        hidden_states = outputs['hidden_states']\n",
    "        vec1 = self._get_cls_vec(hidden_states[-1])\n",
    "        vec2 = self._get_cls_vec(hidden_states[-2])\n",
    "        vec3 = self._get_cls_vec(hidden_states[-3])\n",
    "        vec4 = self._get_cls_vec(hidden_states[-4])\n",
    "        \n",
    "        sequence_output =  torch.cat([vec1, vec2, vec3, vec4], dim=1)  # concatenate\n",
    "        logits = self.regressor(sequence_output)\n",
    "        predict = self.softmax(logits)\n",
    "        return logits,predict,sequence_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623a537-3d2b-4f5f-a166-d2dee909950d",
   "metadata": {},
   "source": [
    "# 解答文と採点結果の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46eb69-df38-4576-884f-e882e97bf556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error,accuracy_score\n",
    "from transformers import AdamW\n",
    "from transformers import BertJapaneseTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "import  warnings\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#何回大元のx解答を変更して検証するか\n",
    "for oomoto_mawasu in range(10):\n",
    "    #ここには位置番号が入ってる\n",
    "    oomoto_kaburi = []\n",
    "    gosaiten_kaburi = []\n",
    "    \n",
    "    #何回誤採点を作るか（そのx回で同じ答案は誤採点に選ばれない）\n",
    "    #現状，ここのx回では毎回大元の答案が変わる\n",
    "    for count_mawasu in range(1):\n",
    "    \n",
    "        subject_list = [\"./rika\",\"./shakai\",\"./kokugo\",\"./sas\",\"./gpt_data/uscis\",\"./gpt_data/seb\",\"./gpt_data/sas\", \"./kanda\", \"./osaka\"]\n",
    "        question_list = [\"/q1\",\"/q2\",\"/q3\",\"/q4\",\"/q5\",\"/q6\",\"/h29_q1\",\"/s3\", \"/kq1\", \"/kq2\", \"/kq3\", \"/kq4\", \"/kq5\", \"/kq6\", \"/kq7\", \"/oq4\", \"/oq7\", \"/oq8\",\"/q7\",\"/q8\", \"/q9\"]\n",
    "    \n",
    "        #print(\"科目は？　(1:理科  2:社会 3:kokugo 4:sas 5:gpt_uscis 6:gpt_seb 7:gpt_sas 8:kanda 9:osaka)\")\n",
    "        subject = subject_list[2]\n",
    "        print()\n",
    "        #print(\"採点する問いは？　(1:q1 2:q2 3:q3 4:q4 5:q5 6:q6 7:tms 8:s3 9:kq1 10:kq2 11:kq3 12:kq4 13:kq5 14:kq6, 15:kq7, 16:oq4 17:oq7 18:oq8)\")\n",
    "        question = question_list[20]\n",
    "        print()\n",
    "    \n",
    "        base_path = subject + question\n",
    "        write_path = base_path + \"/result_total/\"\n",
    "        \n",
    "        print(\"write_path: {}\".format(write_path))\n",
    "    \n",
    "        #学習用解答データ\n",
    "        read_path_text = base_path + question + 'kokugo_train_text.txt'\n",
    "    \n",
    "        #学習用に4倍した解答データ\n",
    "        read_path_text_traingpt = base_path + question + 'kokugo_traingpt_text.txt'\n",
    "    \n",
    "        #誤採点検出用に10倍した解答データ\n",
    "        read_path_text_testgpt = base_path + question + 'kokugo_testgpt_text.txt'\n",
    "    \n",
    "        #学習用得点データ\n",
    "        read_path_score = base_path + question + 'kokugo_train_score.txt'\n",
    "    \n",
    "        #何分割にするか\n",
    "        num = 4\n",
    "        \n",
    "        # ファイルから文章と得点を読み込む\n",
    "        with open(read_path_text) as f:\n",
    "            answer_text_before_all = [str(s.strip()) for s in f.readlines()]\n",
    "            \n",
    "        with open(read_path_text_traingpt) as f:\n",
    "            answer_text_gpt_all = [str(s.strip()) for s in f.readlines()]\n",
    "            \n",
    "        with open(read_path_text_testgpt) as f:\n",
    "            gosaiten_text_all = [str(s.strip()) for s in f.readlines()]\n",
    "    \n",
    "        with open(read_path_score) as f:\n",
    "            graded_score_before_all = [str(s.strip()) for s in f.readlines()]\n",
    "        \n",
    "\n",
    "        \n",
    "        gosaiten_iti = []\n",
    " \n",
    "        def check_common_element(array1, array2):\n",
    "            # フラグの初期値は0\n",
    "            flag = 0\n",
    "            \n",
    "            # 二つの配列を比較し、一致する要素があればフラグを1にする\n",
    "            for element in array1:\n",
    "                if element in array2:\n",
    "                    flag = 1\n",
    "                    break  # 一致する要素が見つかったらループを終了\n",
    "            \n",
    "            # 結果を返す\n",
    "            return flag\n",
    "    \n",
    "    \n",
    "        #読み込んだ全体の解答文（array）からランダムにnum_of_gosaiten個（num_positions個）選ぶ\n",
    "        #全体の解答の選んだ位置の得点を反転させる\n",
    "        def select_random_positions(array, num_positions):\n",
    "            if num_positions > len(array):\n",
    "                print(\"Error: 要素数が不足しています。\")\n",
    "                return\n",
    "    \n",
    "            random_positions = random.sample(range(len(array)), num_positions)\n",
    "            return random_positions\n",
    "        \n",
    "        \n",
    "        num_of_oomoto = 100 #大元の数\n",
    "        kaburiflag = 1\n",
    "        while kaburiflag:\n",
    "            selected_positions = select_random_positions(graded_score_before_all, num_of_oomoto)\n",
    "            \n",
    "            kaburiflag = check_common_element(oomoto_kaburi, selected_positions)\n",
    "            \n",
    "            if kaburiflag == 0:\n",
    "                for k in selected_positions:\n",
    "                    oomoto_kaburi.append(k)\n",
    "        \n",
    "\n",
    "        answer_text_before = []\n",
    "        answer_text_gpt = []\n",
    "        gosaiten_text = []\n",
    "        graded_score_before = []\n",
    "        \n",
    "        #1解答あたり何解答をテスト用（誤採点検出用）として作成したか\n",
    "        x_of_gpt_test = 10\n",
    "        #拡張として何倍作成したか（学習用）\n",
    "        x_of_gpt_train = 4\n",
    "        \n",
    "        for k in selected_positions:\n",
    "            answer_text_before.append(answer_text_before_all[k])\n",
    "            graded_score_before.append(graded_score_before_all[k])\n",
    "            \n",
    "            for l in range(x_of_gpt_train):\n",
    "                answer_text_gpt.append(answer_text_gpt_all[k*x_of_gpt_train+l])\n",
    "                \n",
    "            for l in range(x_of_gpt_test):\n",
    "                gosaiten_text.append(gosaiten_text_all[k*x_of_gpt_test+l])\n",
    "\n",
    "\n",
    "        #kokokaeru\n",
    "        num_of_gosaiten = 5 #誤採点の数\n",
    "        kaburiflag = 1\n",
    "        while kaburiflag == 1:\n",
    "            selected_positions = select_random_positions(graded_score_before, num_of_gosaiten)\n",
    "            print(selected_positions)\n",
    "            kaburiflag = check_common_element(gosaiten_kaburi, selected_positions)\n",
    "    \n",
    "            if kaburiflag == 0:\n",
    "                for k in selected_positions:\n",
    "                    gosaiten_kaburi.append(k)\n",
    "                    \n",
    "        gosaiten_iti = selected_positions\n",
    "    \n",
    "        #全体の解答の大きさのフラグ配列つくり，選んだ位置のフラグを立てる\n",
    "        flag = []\n",
    "        for i in range(len(graded_score_before)):\n",
    "            flag.append(0)\n",
    "        for i in gosaiten_iti:\n",
    "            flag[i] = 1\n",
    "    \n",
    "    \n",
    "        print(\"誤採点に設定した解答とその番号，得点\")\n",
    "        for i in range(len(flag)):\n",
    "            if flag[i] == 1:\n",
    "                print(str(i), answer_text_before[i],\"-\",graded_score_before[i])\n",
    "    \n",
    "        for i in gosaiten_iti:\n",
    "            #kokokaeru\n",
    "            #今回は正解4点，不正解0点なので\n",
    "            if graded_score_before[i] == \"4\":\n",
    "                graded_score_before[i] = \"0\"\n",
    "            elif graded_score_before[i] == \"0\":\n",
    "                graded_score_before[i] = \"4\"\n",
    "                \n",
    "                \n",
    "    \n",
    "    \n",
    "        #gptでx_of_gpt_train倍した分の得点を作る(元を反転後なのでこっちも反転したものがx倍される)\n",
    "        #kokokaeru\n",
    "        x_of_gpt_train = 4\n",
    "        graded_score_gpt = []\n",
    "        for i in graded_score_before:\n",
    "            for k in range(x_of_gpt_train):\n",
    "                graded_score_gpt.append(i)\n",
    "    \n",
    "        \n",
    "        length_1 = len(answer_text_before)\n",
    "    \n",
    "        answer_text_before_n = []\n",
    "        graded_score_before_n = []\n",
    "        \n",
    "        for i in range(num):\n",
    "            if i == 0:\n",
    "                answer_text_before_n.append(answer_text_before[:length_1//num])\n",
    "                graded_score_before_n.append(graded_score_before[:length_1//num])\n",
    "            elif i == num - 1:\n",
    "                answer_text_before_n.append(answer_text_before[i * (length_1//num):])\n",
    "                graded_score_before_n.append(graded_score_before[i * (length_1//num):])\n",
    "            else:\n",
    "                answer_text_before_n.append(answer_text_before[i * (length_1//num):(i+1) * (length_1//num)])\n",
    "                graded_score_before_n.append(graded_score_before[i * (length_1//num):(i+1) * (length_1//num)])\n",
    "        \n",
    "        length_2 = (length_1 // num) * x_of_gpt_train\n",
    "        \n",
    "        answer_text_gpt_n = []\n",
    "        graded_score_gpt_n = []\n",
    "        for i in range(num):\n",
    "            if i == 0:\n",
    "                answer_text_gpt_n.append(answer_text_gpt[:length_2])\n",
    "                graded_score_gpt_n.append(graded_score_gpt[:length_2])\n",
    "            elif i == num - 1:\n",
    "                answer_text_gpt_n.append(answer_text_gpt[i * length_2:])\n",
    "                graded_score_gpt_n.append(graded_score_gpt[i * length_2:])\n",
    "            else:\n",
    "                answer_text_gpt_n.append(answer_text_gpt[i * length_2:(i+1) * length_2])\n",
    "                graded_score_gpt_n.append(graded_score_gpt[i * length_2:(i+1) * length_2])\n",
    "        \n",
    "\n",
    "        length_3 = (length_1 // num) * x_of_gpt_test\n",
    "    \n",
    "        gosaiten_text_n = []\n",
    "        for i in range(num):\n",
    "            if i == 0:\n",
    "                gosaiten_text_n.append(gosaiten_text[:length_3])\n",
    "            elif i == num - 1:\n",
    "                gosaiten_text_n.append(gosaiten_text[i * length_3:])\n",
    "            else:\n",
    "                gosaiten_text_n.append(gosaiten_text[i * length_3:(i+1) * length_3])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        #全体の解答とGPTでx_of_gpt_train倍した解答の文と得点の配列をそれぞれ結合させる\n",
    "        graded_score = []\n",
    "        answer_text = []\n",
    "\n",
    "        for i in range(num):\n",
    "            tmp_graded_score = []\n",
    "            tmp_answer_text = []\n",
    "            for k in range(num):\n",
    "                if i == k:\n",
    "                    continue\n",
    "                else:\n",
    "                    tmp_graded_score += graded_score_before_n[k]\n",
    "                    tmp_graded_score += graded_score_gpt_n[k]\n",
    "                    \n",
    "                    tmp_answer_text += answer_text_before_n[k]\n",
    "                    tmp_answer_text += answer_text_gpt_n[k]\n",
    "            graded_score.append(tmp_graded_score)\n",
    "            answer_text.append(tmp_answer_text)\n",
    "    \n",
    "        #GPTでx_of_gpt_test倍した分の得点を作る(元を反転後なのでこっちも反転したものがx_of_gpt_test倍される)\n",
    "        #こっちはmodelの精度確認用（誤採点検出）\n",
    "        gosaiten_score = []\n",
    "        for i in graded_score_before:\n",
    "            for k in range(x_of_gpt_test):\n",
    "                gosaiten_score.append(i)\n",
    "        \n",
    "        gosaiten_score_n = []\n",
    "        for i in range(num):\n",
    "            if i == 0:\n",
    "                gosaiten_score_n.append(gosaiten_score[:length_3])\n",
    "            elif i == num - 1:\n",
    "                gosaiten_score_n.append(gosaiten_score[i * length_3:])\n",
    "            else:\n",
    "                gosaiten_score_n.append(gosaiten_score[i * length_3:(i+1) * length_3])\n",
    "        \n",
    "        \n",
    "  \n",
    "        \n",
    "        #エポック数の指定\n",
    "        for num_of_epoch in range(1,6):\n",
    "            loop_num = 1  # 実験の試行回数 モデルをいくつ作るか\n",
    "            make_traindata_method = 2 # 訓練データの作り方は？　(1:クラスタの中心　2:ランダム)\n",
    "        \n",
    "            # クラスタ数（学習データ数）を決める\n",
    "            # 1以上にするとk-means、１未満だと階層型クラスタリングを行う\n",
    "            # WARNING: 現状、１未満にすると学習データが0個になってしまう\n",
    "            cluster_num =  350\n",
    "            BATCH_SIZE = 10\n",
    "            if make_traindata_method == 1:\n",
    "                if (cluster_num >= 1):\n",
    "                    # バッチサイズごとに分けた場合、データが1個余るとエラーになる\n",
    "                    if (cluster_num % BATCH_SIZE == 1 or (answer_num_gpt - cluster_num) % BATCH_SIZE == 1):\n",
    "                        cluster_num += 1\n",
    "        \n",
    "        \n",
    "            # モデルのパラメータを記述\n",
    "            EMBEDDING_DIM = 768\n",
    "            HIDDEN_DIM = 128\n",
    "            TAG_SIZE = 2\n",
    "        \n",
    "        \n",
    "            make_traindata_method = 2\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "        \n",
    "            start = time.time()\n",
    "        \n",
    "            #kokokaeru\n",
    "            model_name = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "            #model_name = \"bert-base-uncased\"\n",
    "            #model_name = \"bert-base-cased\"\n",
    "            #\"bert-base-japanese-v2\"\n",
    "            #\"bert-base-japanese-whole-word-masking\"\n",
    "            #\"bert-base-japanese-v2\"\n",
    "        \n",
    "            \n",
    "        \n",
    "            for num_mawasi in range(num):\n",
    "                loop_count = 0\n",
    "                acc_list = []\n",
    "                answer_num = len(graded_score[num_mawasi])\n",
    "                print(\"----- {} -----\".format(base_path))\n",
    "        \n",
    "                while(loop_count < loop_num):\n",
    "                    loop_count += 1\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(model_name,is_fast=True)\n",
    "                    \n",
    "                    qwk_act = []\n",
    "                    acc_act = []\n",
    "                    #cluster_num0は学習データ数\n",
    "                    cluster_num0 = answer_num\n",
    "                    train_num = cluster_num0\n",
    "        \n",
    "                    \n",
    "                    print(\"全解答数：　\" + str(answer_num))\n",
    "                    count_list = [0, 0]\n",
    "                    for score in num_to_num(graded_score[num_mawasi]):\n",
    "                        if score == 0:\n",
    "                            count_list[0] += 1\n",
    "                        elif score == 1:\n",
    "                            count_list[1] += 1\n",
    "                    \n",
    "                    print(\"all data: [0, 1]={}\".format(count_list))\n",
    "        \n",
    "                    # インデックスのリストで代入できるように変換する（処理を早くするため）\n",
    "                    graded_score_gakusyu = np.array(graded_score[num_mawasi])\n",
    "                    answer_text_gakusyu = np.array(answer_text[num_mawasi])\n",
    "                    #print(graded_score_gakusyu)\n",
    "                    #print(answer_text_gakusyu)\n",
    "                    \n",
    "                    train_vec = []\n",
    "                    train_seq_vec = []\n",
    "        \n",
    "                    if make_traindata_method == 1:\n",
    "                        print('クラスタの中心の解答を学習データにする')\n",
    "                        print()\n",
    "                        \n",
    "                        # クラスタリングをする\n",
    "                        index_list = clustering(cls_token_vec, answer_text_gakusyu, cluster_num)\n",
    "                        \n",
    "                    elif make_traindata_method == 2:\n",
    "                        print('ランダムに解答を学習データにする')\n",
    "                        print()\n",
    "        \n",
    "                        # インデックスをシャッフルする\n",
    "                        np.random.seed(loop_count)\n",
    "                        index_list = np.random.permutation(answer_num)\n",
    "                    elif make_traindata_method == 3:\n",
    "                        print(\"類似度で分類\")\n",
    "                        print()\n",
    "                        index_list = clustering_cos(cls_token_vec,model_cls_vec,cluster_num)\n",
    "                    elif make_traindata_method == 4:\n",
    "                        print(\"ベクトル+類似度で分類\")\n",
    "                        print()\n",
    "                        index_list = clustering_dis_cos(cls_token_vec,model_cls_vec,cluster_num)\n",
    "                        \n",
    "        \n",
    "                    print(\"データを分ける\")\n",
    "                    encoded_data = tokenizer.batch_encode_plus(answer_text_gakusyu.tolist(), pad_to_max_length=True, add_special_tokens=True)\n",
    "        \n",
    "                    train_score = graded_score_gakusyu[index_list[:train_num]]\n",
    "                    train_score = train_score.tolist()\n",
    "                    train_text = answer_text_gakusyu[index_list[:train_num]]\n",
    "                    train_text = train_text.tolist()\n",
    "                    train_input_ids = torch.tensor(encoded_data[\"input_ids\"])[index_list[:train_num]]\n",
    "                    train_attention_mask = torch.tensor(encoded_data[\"attention_mask\"])[index_list[:train_num]]\n",
    "                    train_token_type_ids = torch.tensor(encoded_data[\"token_type_ids\"])[index_list[:train_num]]\n",
    "                    train_score_num = num_to_num(train_score)\n",
    "                    print(\"学習データ\")\n",
    "                    \n",
    "                    print('train_vec:{}, train_score:{}, train_text:{}, train_seq_vec:{}'.format(len(train_vec), len(train_score), len(train_text) ,len(train_seq_vec)))\n",
    "                    print()\n",
    "        \n",
    "                    \n",
    "                    count_list = [0, 0]\n",
    "                    for score in train_score_num:\n",
    "                        if score == 0:\n",
    "                            count_list[0] += 1\n",
    "                        elif score == 1:\n",
    "                            count_list[1] += 1\n",
    "        \n",
    "                    \n",
    "                    print(\"train data: [0, 1]={}\".format(count_list))\n",
    "                    lr_list = [5e-5,4e-5,3e-5,2e-5]\n",
    "                    #lr_list = [5e-5]\n",
    "                    learning_rates = 5e-5\n",
    "                    max_qwk = 0\n",
    "                    min_loss = 1000000000\n",
    "                    for lr_count,LR in enumerate(lr_list):\n",
    "        \n",
    "                        model =  CommonLitModel(model_name,r = TAG_SIZE).to(device)  # to(device)でモデルがGPU対応する\n",
    "        \n",
    "                        no_decay = ['bias', 'LayerNorm.weight']\n",
    "                        optimizer_grouped_parameters = [\n",
    "                            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                        ]\n",
    "                        optimizer = AdamW(optimizer_grouped_parameters, lr=LR)\n",
    "        \n",
    "                        model_path = base_path + \"/model/model_num{}_{}.pth\".format(num_mawasi+1, loop_count) # modelの保存先\n",
    "        \n",
    "                        loss_function = nn.CrossEntropyLoss()\n",
    "                        \n",
    "                        loss_epoch = []\n",
    "                        acc = []\n",
    "        \n",
    "                        num_training_steps = ((train_num// BATCH_SIZE)+1)*5\n",
    "                        scheduler = get_linear_schedule_with_warmup(optimizer, int(0.1*num_training_steps), num_training_steps)\n",
    "                        # 学習\n",
    "                        for epoch in range(num_of_epoch):\n",
    "                            model.train() #学習モード\n",
    "        \n",
    "                            # バッチ化\n",
    "                            text_batch = []\n",
    "                            input_ids_batch = []\n",
    "                            attention_mask_batch = []\n",
    "                            token_type_ids_batch = []\n",
    "                            category_batch = []\n",
    "                            for i in range(0, train_num, BATCH_SIZE):\n",
    "                                text_batch.append(train_text[i:i+BATCH_SIZE])\n",
    "                                input_ids_batch.append(train_input_ids[i:i+BATCH_SIZE])\n",
    "                                attention_mask_batch.append(train_attention_mask[i:i+BATCH_SIZE])\n",
    "                                token_type_ids_batch.append(train_token_type_ids[i:i+BATCH_SIZE])\n",
    "                                category_batch.append(train_score_num[i:i+BATCH_SIZE])\n",
    "        \n",
    "                            all_loss = 0\n",
    "                            all_acc = 0\n",
    "        \n",
    "                            predict_list = []\n",
    "                            for i in range(len(text_batch)):\n",
    "        \n",
    "                                batch_loss = 0\n",
    "        \n",
    "                                # 順伝搬させるtensorはGPUで処理させるためdevice=にGPUをセット\n",
    "        \n",
    "                                # category_tensor.size() = (batch_size × 1)なので、squeeze()\n",
    "                                input_ids_tensor = torch.tensor(input_ids_batch[i], device=device)\n",
    "                                attention_mask_tensor = torch.tensor(attention_mask_batch[i], device=device)\n",
    "                                token_type_ids_tensor = torch.tensor(token_type_ids_batch[i], device=device)\n",
    "                                category_tensor = torch.tensor(category_batch[i], device=device).squeeze()\n",
    "        \n",
    "                                logits,out,_ = model(input_ids_tensor,attention_mask=attention_mask_tensor,token_type_ids=token_type_ids_tensor)\n",
    "                                model.zero_grad()\n",
    "                                optimizer.zero_grad()\n",
    "        \n",
    "                                batch_loss = loss_function(logits, category_tensor)\n",
    "                                batch_loss.backward()\n",
    "        \n",
    "                                optimizer.step()\n",
    "                                scheduler.step()\n",
    "        \n",
    "                                all_loss += batch_loss.item()\n",
    "                                predict_list.extend(num_to_num_after(torch.argmax(out,1)))\n",
    "        \n",
    "                            qwk = cohen_kappa_score(predict_list, train_score_num, weights='quadratic')\n",
    "        \n",
    "                            loss_epoch.append(all_loss)\n",
    "                            acc.append(qwk)\n",
    "        \n",
    "                            print(\"epoch {: >3} | train_loss:{:.4f} , train_acc:{:.4f}\".format(epoch+1, all_loss, qwk))\n",
    "                            if(min_loss > all_loss):\n",
    "                                print(\"Loss: save new weight at {} epoch\".format(epoch+1))\n",
    "                                torch.save(model.state_dict(), model_path) \n",
    "                                min_loss = all_loss\n",
    "                                learning_rates = LR\n",
    "                                losses_epoch = loss_epoch\n",
    "                                accs = acc\n",
    "                                \n",
    "                                # dev_losses_epoch = dev_loss_epoch\n",
    "                                # dev_accs = dev_acc\n",
    "                            #torch.save(model.state_dict(), model_path)\n",
    "                    \n",
    "                    print(\"-------------------------------------------------------------------------\")\n",
    "                    print(\"|                                       \"+ str(num_mawasi+1) + \"の\" + str(loop_count) + \"回目終了！\")\n",
    "                    print(\"-------------------------------------------------------------------------\\n\")\n",
    "                    # ----------------------------------------------------------loopの終わり---------------------------------------------------------\n",
    "        \n",
    "            elapsed_time = (time.time() - start) / 60\n",
    "        \n",
    "            print (\"elapsed_time:{:.2f}\".format(elapsed_time) + \"[min]\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            #誤採点検出\n",
    "        \n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "        \n",
    "            loop_count = 0\n",
    "            acc_list = []\n",
    "            predict_list_mtm = []\n",
    "        \n",
    "            while(loop_count < loop_num):\n",
    "                loop_count += 1\n",
    "                predict_list_num_mawasi = []\n",
    "                for num_mawasi in range(num):\n",
    "                    \n",
    "                    \n",
    "                    tokenizer = AutoTokenizer.from_pretrained(model_name,is_fast=True)        \n",
    "                    \n",
    "                    answer_num = len(gosaiten_score_n[num_mawasi])\n",
    "                    print(\"全解答数：　\" + str(answer_num))\n",
    "                    count_list = [0, 0]\n",
    "                    for score in num_to_num(gosaiten_score_n[num_mawasi]):\n",
    "                        if score == 0:\n",
    "                            count_list[0] += 1\n",
    "                        elif score == 1:\n",
    "                            count_list[1] += 1\n",
    "                    \n",
    "                    print(\"all data: [0, 1]={}\".format(count_list))\n",
    "        \n",
    "                    # インデックスのリストで代入できるように変換する（処理を早くするため）\n",
    "                    gosaiten_score_test = np.array(gosaiten_score_n[num_mawasi])\n",
    "                    gosaiten_text_test = np.array(gosaiten_text_n[num_mawasi])\n",
    "                    print(\"データを分ける\")\n",
    "                    encoded_data = tokenizer.batch_encode_plus(gosaiten_text_test.tolist(), pad_to_max_length=True, add_special_tokens=True)\n",
    "        \n",
    "                    test_text = gosaiten_text_test\n",
    "                    test_score = gosaiten_score_test\n",
    "        \n",
    "                    \n",
    "                    test_input_ids = torch.tensor(encoded_data[\"input_ids\"])\n",
    "                    test_attention_mask = torch.tensor(encoded_data[\"attention_mask\"])\n",
    "                    test_token_type_ids = torch.tensor(encoded_data[\"token_type_ids\"])\n",
    "                    test_score_num = num_to_num(test_score)\n",
    "                        \n",
    "                    index_list = np.random.permutation(len(test_text))\n",
    "                    \n",
    "                    test_num = answer_num\n",
    "                    \n",
    "                        \n",
    "        \n",
    "                    # テストデータ\n",
    "                    test_score = np.array(test_score).tolist()\n",
    "                    test_text = np.array(test_text).tolist()\n",
    "                \n",
    "                    model =  CommonLitModel(model_name, r = TAG_SIZE).to(device)\n",
    "        \n",
    "                    #kokokaeru\n",
    "                    model_path = base_path + \"/model/model_num{}_{}.pth\".format(num_mawasi+1,loop_count)\n",
    "                    print(model_path)\n",
    "                    # test\n",
    "                    model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        \n",
    "                    # バッチ化\n",
    "                    text_batch = []\n",
    "                    test_cls = []\n",
    "                    test_un = []\n",
    "                    input_ids_batch = []\n",
    "                    attention_mask_batch = []\n",
    "                    token_type_ids_batch = []\n",
    "                    category_batch = []\n",
    "        \n",
    "                    for i in range(0, test_num, BATCH_SIZE):\n",
    "                        text_batch.append(test_text[i:i+BATCH_SIZE])\n",
    "                        input_ids_batch.append(test_input_ids[i:i+BATCH_SIZE])\n",
    "                        attention_mask_batch.append(test_attention_mask[i:i+BATCH_SIZE])\n",
    "                        token_type_ids_batch.append(test_token_type_ids[i:i+BATCH_SIZE])\n",
    "                        category_batch.append(test_score_num[i:i+BATCH_SIZE])\n",
    "        \n",
    "                    \n",
    "                    predict_list = []\n",
    "                    for i in range(len(text_batch)):\n",
    "        \n",
    "                        input_ids_tensor = torch.tensor(input_ids_batch[i], device=device)\n",
    "                        attention_mask_tensor = torch.tensor(attention_mask_batch[i], device=device)\n",
    "                        token_type_ids_tensor = torch.tensor(token_type_ids_batch[i], device=device)\n",
    "                        category_tensor = torch.tensor(category_batch[i], device=device).squeeze()\n",
    "                        logits,out,out1 = model(input_ids_tensor,attention_mask_tensor,token_type_ids_tensor)\n",
    "        \n",
    "                        test_un.extend(out.tolist())\n",
    "                        test_cls.extend(out1.tolist())\n",
    "        \n",
    "                        predict_list.extend(num_to_num_after(torch.argmax(out,1)))\n",
    "                    predict_list_num_mawasi.append(predict_list)\n",
    "        \n",
    "                predict_list_mtm.append(sum(predict_list_num_mawasi,[])) \n",
    "        \n",
    "                acc_list.append(accuracy_score(predict_list_mtm[loop_count-1], num_to_num(sum(gosaiten_score_n,[]) )))\n",
    "                \n",
    "                print(\"acc_list\",acc_list)\n",
    "        \n",
    "                print(\"-------------------------------------------------------------------------\")\n",
    "                print(\"|                                       \"+ str(loop_count) + \"の\" + str(num_mawasi+1) + \"回目終了！\")\n",
    "                print(\"-------------------------------------------------------------------------\\n\")\n",
    "                # ----------------------------------------------------------loopの終わり--------------------------------------------------------\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            list_of_miss_count_list_ave = []\n",
    "            list_of_true_miss_count_list_ave = []\n",
    "        \n",
    "            fuitti_count_max = 1\n",
    "        \n",
    "            graded_score_before_copy = []\n",
    "        \n",
    "            for i in graded_score_before:\n",
    "                graded_score_before_copy.append(i)\n",
    "        \n",
    "        \n",
    "            for n in range(len(graded_score_before_copy)):\n",
    "                if graded_score_before_copy[n] != '0':\n",
    "                    graded_score_before_copy[n] = 1\n",
    "                else:\n",
    "                    graded_score_before_copy[n] = 0\n",
    "        \n",
    "        \n",
    "            print(len(predict_list_mtm[0]))\n",
    "\n",
    "            kensyutu_dekita_touan_list = []\n",
    "            #GPTでx_of_gpt_test倍に拡張したから最大でもfuitti_count_maxはx_of_gpt_testだから\n",
    "            for fuitti_turn in range(x_of_gpt_test):\n",
    "                print(\"fuitti_count_max=\",fuitti_count_max)\n",
    "                \n",
    "                miss_count_list_ave = 0\n",
    "                true_miss_count_list_ave = 0\n",
    "                #今回はmodelがloop_num個だから\n",
    "                for model_turn in range(loop_num):\n",
    "                    print(\"model_name=\",model_turn+1)\n",
    "                    # モデル絞る時は下のコメントアウト解除，割る数も変える（下のところ）\n",
    "                    # if (model_turn+1) != 1:\n",
    "                    #     continue\n",
    "                    \n",
    "                    #GPTの予測結果\n",
    "                    yosoku_kekka = predict_list_mtm[model_turn]\n",
    "                    miss_count_list = []\n",
    "                    true_miss_count_list = []\n",
    "                    \n",
    "                    #今回は誤採点リストを1作ったから\n",
    "                    for i in range(1):\n",
    "                        \n",
    "                        #誤採点と認識したものカウント\n",
    "                        miss_count = 0\n",
    "                        #そのうちの真誤採点カウント\n",
    "                        true_miss_count = 0\n",
    "                        m = 0\n",
    "                        for k in range(len(graded_score_before_copy)):\n",
    "                            #不一致カウント\n",
    "                            fuitti_count = 0\n",
    "                            #今回はGPTでx_of_gpt_test倍に拡張したから\n",
    "                            for l in range(x_of_gpt_test):\n",
    "                                if graded_score_before_copy[k] != yosoku_kekka[m]:\n",
    "                                    fuitti_count+=1\n",
    "                                m+=1\n",
    "                            if fuitti_count >= fuitti_count_max:\n",
    "                                miss_count += 1\n",
    "                                if int(flag[k]) == 1:\n",
    "                                    print(answer_text_before[k])\n",
    "                                    true_miss_count += 1\n",
    "                                    ####\n",
    "                                    if fuitti_count_max == 1:\n",
    "                                        kensyutu_dekita_touan_list.append(answer_text_before[k])\n",
    "                        miss_count_list.append(miss_count)\n",
    "                        true_miss_count_list.append(true_miss_count)\n",
    "                        miss_count_list_ave += miss_count\n",
    "                        true_miss_count_list_ave += true_miss_count\n",
    "        \n",
    "                    print(miss_count_list)\n",
    "                    print(true_miss_count_list) \n",
    "                    \n",
    "                #今回は誤採点リストを1作り，modelはloop_num個だから\n",
    "                miss_count_list_ave /= loop_num\n",
    "                true_miss_count_list_ave /= loop_num\n",
    "                list_of_miss_count_list_ave.append(miss_count_list_ave)\n",
    "                list_of_true_miss_count_list_ave.append(true_miss_count_list_ave)\n",
    "        \n",
    "                print()\n",
    "                print(miss_count_list_ave)\n",
    "                print(true_miss_count_list_ave) \n",
    "                print()   \n",
    "                fuitti_count_max += 1   \n",
    "                \n",
    "            write_result_path = base_path + \"kekka_\"+str(count_mawasu+1)+\"oomoto_\"+str(oomoto_mawasu+1)+\"-bestepoch\"+str(num_of_epoch)+\".txt\"\n",
    "            with open(write_result_path, mode='w') as f:\n",
    "                f.write(\"誤採点の一覧\\n\")\n",
    "                for i in range(len(flag)):\n",
    "                    if flag[i] == 1:\n",
    "                        f.write(str(i)+\":\"+answer_text_before[i]+\"\\n\")\n",
    "                        \n",
    "                f.write(\"検出できた誤採点\\n\")\n",
    "                for i in range(len(kensyutu_dekita_touan_list)):\n",
    "                    f.write(kensyutu_dekita_touan_list[i]+\"\\n\")\n",
    "                    \n",
    "                f.write(\"検出数\\n\")\n",
    "                for i in range(len(list_of_miss_count_list_ave)):\n",
    "                    f.write(str(list_of_miss_count_list_ave[i])+\"\\n\")\n",
    "                    \n",
    "                f.write(\"検出したうち本当の誤採点数\\n\")\n",
    "                for i in range(len(list_of_true_miss_count_list_ave)):\n",
    "                    f.write(str(list_of_true_miss_count_list_ave[i])+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
